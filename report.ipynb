{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza zmiany wartości mid price na podstawie Limit Order Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy LOBy analizować \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imbalance import prep_data\n",
    "from parser import parse\n",
    "from utils import confusion_matrix\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "file_locs = ['data/OrderBookSnapshots.9061.csv', 'data/OrderBookSnapshots.9062.csv', 'data/OrderBookSnapshots.9063.csv',\n",
    "             'data/OrderBookSnapshots.9064.csv', 'data/OrderBookSnapshots.9065.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No to z czym sie będziemy porównywać to imbalance z pracy (link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5510204081632653\n",
      "[[260 207]\n",
      " [211 253]]\n",
      "\n",
      "0.5250800426894343\n",
      "[[200 177]\n",
      " [268 292]]\n",
      "\n",
      "0.5465631929046563\n",
      "[[216 154]\n",
      " [255 277]]\n",
      "\n",
      "0.5159629248197735\n",
      "[[169 166]\n",
      " [304 332]]\n",
      "\n",
      "0.5551181102362205\n",
      "[[169 119]\n",
      " [220 254]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "for file_name in file_locs:\n",
    "    X_train, Y_train, X_test, Y_test = prep_data(parse(file_name))\n",
    "    classifier = linear_model.SGDClassifier(loss=\"log\", alpha=0.1, max_iter=3000, tol=0, shuffle=False)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_check = classifier.predict(X_test)\n",
    "    print(classifier.score(X_test,Y_test))\n",
    "    print(confusion_matrix(Y_check, Y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikator oparty na sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nie wiem czy coś tu pisać"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast brać słupki z LOB dzielimy cały zakres na kubełki, których wielkość to ułamek wartości mid price. Następnie bierzemy tylko kilka z każdej strony mid price i normujemy tak, aby wartości sumowały się do jedynki.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plik preprocess.py\n",
    "\n",
    "def fits(index, arr):\n",
    "    return index >= 0 and index < len(arr)\n",
    "\n",
    "\n",
    "def get_XY(data, n_buckets=5, bucket_size=0.05 , omit_no_change=True):\n",
    "    \n",
    "    keys = list(data.keys())\n",
    "    keys.sort()\n",
    "\n",
    "    \n",
    "    bad0 = 0\n",
    "    bad1 = 0\n",
    "\n",
    "    growths = []\n",
    "    X = []\n",
    "\n",
    "    for i, curr in enumerate(keys[:-1]):\n",
    "        currKey = curr\n",
    "        nextKey = keys[i+1]\n",
    "\n",
    "        currday, currh = currKey.split()\n",
    "        nextday, nexth = nextKey.split()\n",
    "\n",
    "        currh = int(currh[:4])\n",
    "        nexth = int(nexth[:4])\n",
    "\n",
    "        \n",
    "\n",
    "        if currh < 900 or currh > 1600 or currh + 1 != nexth:\n",
    "            bad0 += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        if not omit_no_change or data[nextKey][2] != data[currKey][2]:\n",
    "            rows0 = np.zeros(2*n_buckets)\n",
    "            rows = np.zeros(2*n_buckets)\n",
    "            \n",
    "            mid_price = data[currKey][2]\n",
    "            centers = (np.arange(2*n_buckets) - n_buckets + 0.5)*bucket_size*mid_price + mid_price\n",
    "            #print(mid_price, centers)\n",
    "\n",
    "            #print(mid_price)\n",
    "            for bid_price, bid_size in reversed(data[currKey][0]):\n",
    "                bucket = int(( n_buckets*bucket_size -  (mid_price-bid_price)/mid_price)/bucket_size)\n",
    "                #print(bucket)\n",
    "                if fits(bucket, rows):\n",
    "                    rows0[bucket] += bid_size\n",
    "                    #norm += bid_size\n",
    "                    norm = 0.\n",
    "                    for i in range(len(rows)//2):\n",
    "                        norm += 1. / max(1.*(abs(centers[i]-bid_price)/(mid_price*bucket_size))**2., 0.25)\n",
    "                    check = 0.\n",
    "                    for i in range(len(rows)//2):\n",
    "                        rows[i] += bid_size* ( (1./max((abs(centers[i]-bid_price)/(mid_price*bucket_size))**2., 0.25)) / norm )\n",
    "                        check += ( (1./max(1.*(abs(centers[i]-bid_price)/(mid_price*bucket_size))**2., 0.25)) / norm )\n",
    "\n",
    "            for ask_price, ask_size in data[currKey][1]:\n",
    "                bucket = int(((ask_price - mid_price)/mid_price )/bucket_size)\n",
    "                #print(bucket + n_buckets)\n",
    "                if fits(bucket + n_buckets, rows):\n",
    "                    rows0[bucket + n_buckets] += ask_size\n",
    "                    #norm += bid_size\n",
    "                    norm = 0.\n",
    "                    for i in range(len(rows)//2, len(rows)):\n",
    "                        norm += 1. / max(1.*(abs(centers[i]-ask_price)/(mid_price*bucket_size))**2., 0.25)\n",
    "                    for i in range(len(rows)//2, len(rows)):\n",
    "                        rows[i] += ask_size* ( (1./max(1.*(abs(centers[i]-ask_price)/(mid_price*bucket_size))**2., 0.25)) / norm )\n",
    "\n",
    "            # poprawne dane - min ask > max bid\n",
    "            if data[currKey][0][-1][0] <  data[currKey][1][0][0] and data[nextKey][0][-1][0] <  data[nextKey][1][0][0]:\n",
    "                growths.append(data[currKey][2] < data[nextKey][2])\n",
    "                rows /= rows.sum() #norm\n",
    "                rows0 /= rows0.sum()\n",
    "                X.append(rows)\n",
    "            else:\n",
    "                bad1 += 1\n",
    "\n",
    "\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    Y = np.array(growths, dtype=np.int)\n",
    "\n",
    "    test_size =  len(Y)//8\n",
    "\n",
    "    #dzielenie na czesc do uczenia/walidacji i testowa\n",
    "    return (X[test_size:], Y[test_size:]), (X[:test_size], Y[:test_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do trenowania używamy klasycznej sieci neuronowej z warstwami fc i dodatkowo dropoutem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size = 0.04\n",
    "input_size = 2*3\n",
    "hidden_size = 1000\n",
    "\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.Tanh(),\n",
    "    nn.Dropout(p=0.6, inplace=False),\n",
    "    nn.Linear(hidden_size, hidden_size//4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size//4, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6941564679145813\n",
      "Acc: 0.5064655172413793\n",
      "Confusion matrix: \n",
      "[[  0   0]\n",
      " [229 235]]\n",
      "\n",
      "Loss: 0.6927377581596375\n",
      "Acc: 0.5161987041036717\n",
      "Confusion matrix: \n",
      "[[  0   0]\n",
      " [224 239]]\n",
      "\n",
      "Loss: 0.7168537974357605\n",
      "Acc: 0.49774774774774777\n",
      "Confusion matrix: \n",
      "[[  0   0]\n",
      " [223 221]]\n",
      "\n",
      "Loss: 0.6917941570281982\n",
      "Acc: 0.5269709543568465\n",
      "Confusion matrix: \n",
      "[[211 196]\n",
      " [ 32  43]]\n",
      "\n",
      "Loss: 0.6923332214355469\n",
      "Acc: 0.5108108108108108\n",
      "Confusion matrix: \n",
      "[[  0   0]\n",
      " [181 189]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import accuracy\n",
    "\n",
    "from preprocess import get_XY\n",
    "\n",
    "for data in file_locs:\n",
    "    _, (X, Y) = get_XY(parse(data), n_buckets=input_size//2, bucket_size=bucket_size)\n",
    "    \n",
    "    set_number = data[data.find('.')+1:data.rfind('.')]\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"models/model\"+set_number))\n",
    "    model.eval()\n",
    "    X = torch.from_numpy(X)\n",
    "    Y = torch.from_numpy(Y)\n",
    "    pred = model.forward(X)\n",
    "    loss_val = loss(pred, Y)\n",
    "    print(\"Loss: {}\".format(loss_val.item()))\n",
    "    print(\"Acc: {}\".format(accuracy(pred.data.numpy(), Y.data.numpy())))\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(confusion_matrix(np.argmax(pred.data.numpy(), axis=1), Y.data.numpy()))\n",
    "    print()\n",
    "    \n",
    "## python3 main.py --data ../data/OrderBookSnapshots.9062.csv --debug --epochs 250 --buckets 3 --bucket_size 0.04 --print_every 100 --hidden 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dziękujemy za uwagę"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
