{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza zmiany wartości mid price na podstawie Limit Order Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy LOBy analizować \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imbalance import prep_data\n",
    "from parser import parse\n",
    "from utils import confusion_matrix\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "file_locs = ['data/OrderBookSnapshots.9061.csv', 'data/OrderBookSnapshots.9062.csv', 'data/OrderBookSnapshots.9063.csv',\n",
    "             'data/OrderBookSnapshots.9064.csv', 'data/OrderBookSnapshots.9065.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No to z czym sie będziemy porównywać to imbalance z pracy (link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5435016111707841\n",
      "[[226 160]\n",
      " [265 280]]\n",
      "\n",
      "0.5314834578441836\n",
      "[[195 164]\n",
      " [275 303]]\n",
      "\n",
      "0.5532150776053215\n",
      "[[263 242]\n",
      " [161 236]]\n",
      "\n",
      "0.553038105046344\n",
      "[[149 101]\n",
      " [333 388]]\n",
      "\n",
      "0.5866141732283464\n",
      "[[199 136]\n",
      " [179 248]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "for file_name in file_locs:\n",
    "    X_train, Y_train, X_test, Y_test = prep_data(parse(file_name))\n",
    "    classifier = linear_model.SGDClassifier(loss=\"log\", alpha=0.1, max_iter=3000, tol=0, shuffle=False)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_check = classifier.predict(X_test)\n",
    "    print(classifier.score(X_test,Y_test))\n",
    "    print(confusion_matrix(Y_check, Y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikator oparty na sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nie wiem czy coś tu pisać"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast brać słupki z LOB dzielimy cały zakres na kubełki, których wielkość to ułamek wartości mid price. Następnie bierzemy tylko kilka z każdej strony mid price i normujemy tak, aby wartości sumowały się do jedynki.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fits(index, arr):\n",
    "    return index >= 0 and index < len(arr)\n",
    "\n",
    "\n",
    "def get_XY(data, n_buckets=5, bucket_size=0.05 , omit_no_change=True):\n",
    "    \n",
    "    keys = list(data.keys())\n",
    "    keys.sort()\n",
    "\n",
    "    growths = []\n",
    "    X = []\n",
    "\n",
    "    for i, curr in enumerate(keys[:-1]):\n",
    "        currKey = curr\n",
    "        nextKey = keys[i+1]\n",
    "\n",
    "        if not omit_no_change or data[nextKey][2] != data[currKey][2]:\n",
    "            rows = np.zeros(2*n_buckets)\n",
    "            mid_price = data[currKey][2]\n",
    "            #print(mid_price)\n",
    "            for bid_price, bid_size in reversed(data[currKey][0]):\n",
    "                bucket = int(( n_buckets*bucket_size -  (mid_price-bid_price)/mid_price)/bucket_size)\n",
    "                #print(bucket)\n",
    "                if fits(bucket, rows):\n",
    "                    rows[bucket] += bid_size\n",
    "\n",
    "            for ask_price, ask_size in data[currKey][1]:\n",
    "                bucket = int(((ask_price - mid_price)/mid_price )/bucket_size)\n",
    "                #print(bucket + n_buckets)\n",
    "                if fits(bucket + n_buckets, rows):\n",
    "                    rows[bucket + n_buckets] += ask_size\n",
    "\n",
    "            ###print(\"!!!\", currKey, nextKey)\n",
    "\n",
    "            # poprawne dane - min ask > max bid\n",
    "            if data[currKey][0][-1][0] <  data[currKey][1][0][0] and data[nextKey][0][-1][0] <  data[nextKey][1][0][0]:\n",
    "                growths.append(data[currKey][2] < data[nextKey][2])\n",
    "                rows /= rows.sum()\n",
    "                X.append(rows)\n",
    "                \n",
    "    return np.array(X, dtype=np.float32), np.array(growths, dtype=np.int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do trenowania używamy klasycznej sieci neuronowej z warstwami fc i dodatkowo dropoutem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size = 0.04\n",
    "input_size = 2*3\n",
    "hidden_size = 1000\n",
    "\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size),\n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.6, inplace=False),\n",
    "        nn.Linear(hidden_size, hidden_size//4),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size//4, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6941971182823181\n",
      "Acc: 0.5\n",
      "Confusion matrix: \n",
      "[[  76   76]\n",
      " [1780 1780]]\n",
      "\n",
      "Loss: 0.6952551007270813\n",
      "Acc: 0.4825910931174089\n",
      "Confusion matrix: \n",
      "[[1281 1376]\n",
      " [ 541  507]]\n",
      "\n",
      "Loss: 0.693829357624054\n",
      "Acc: 0.5049254151421334\n",
      "Confusion matrix: \n",
      "[[1386 1386]\n",
      " [ 373  408]]\n",
      "\n",
      "Loss: 0.6929921507835388\n",
      "Acc: 0.512707468879668\n",
      "Confusion matrix: \n",
      "[[1128 1126]\n",
      " [ 753  849]]\n",
      "\n",
      "Loss: 0.6936193108558655\n",
      "Acc: 0.5059021922428331\n",
      "Confusion matrix: \n",
      "[[935 928]\n",
      " [537 565]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import accuracy\n",
    "\n",
    "from preprocess import get_XY\n",
    "\n",
    "for data in file_locs:\n",
    "    X, Y = get_XY(parse(data), n_buckets=input_size//2, bucket_size=bucket_size)\n",
    "    \n",
    "    set_number = data[data.find('.')+1:data.rfind('.')]\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"models/model\"+set_number))\n",
    "    model.eval()\n",
    "    X = torch.from_numpy(X)\n",
    "    Y = torch.from_numpy(Y)\n",
    "    pred = model(X)\n",
    "    loss_val = loss(pred, Y)\n",
    "    print(\"Loss: {}\".format(loss_val.item()))\n",
    "    print(\"Acc: {}\".format(accuracy(pred.data.numpy(), Y.data.numpy())))\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(confusion_matrix(np.argmax(pred.data.numpy(), axis=1), Y.data.numpy()))\n",
    "    print()\n",
    "    \n",
    "## python3 main.py --data ../data/OrderBookSnapshots.9062.csv --debug --epochs 250 --buckets 3 --bucket_size 0.04 --print_every 100 --hidden 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dziękujemy za uwagę"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
