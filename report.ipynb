{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza zmiany wartości mid price na podstawie Limit Order Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy LOBy analizować \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imbalance import prep_data\n",
    "from parser import parse\n",
    "from utils import confusion_matrix\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "file_locs = ['data/OrderBookSnapshots.9061.csv', 'data/OrderBookSnapshots.9062.csv', 'data/OrderBookSnapshots.9063.csv',\n",
    "             'data/OrderBookSnapshots.9064.csv', 'data/OrderBookSnapshots.9065.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No to z czym sie będziemy porównywać to imbalance z pracy (link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.564983888292159\n",
      "[[266 199]\n",
      " [206 260]]\n",
      "\n",
      "0.5240128068303095\n",
      "[[181 153]\n",
      " [293 310]]\n",
      "\n",
      "0.5776053215077606\n",
      "[[233 139]\n",
      " [242 288]]\n",
      "\n",
      "0.5417095777548918\n",
      "[[170 140]\n",
      " [305 356]]\n",
      "\n",
      "0.5695538057742782\n",
      "[[184 134]\n",
      " [194 250]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "for file_name in file_locs:\n",
    "    X_train, Y_train, X_test, Y_test = prep_data(parse(file_name))\n",
    "    classifier = linear_model.SGDClassifier(loss=\"log\", alpha=0.1, max_iter=3000, tol=0, shuffle=False)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_check = classifier.predict(X_test)\n",
    "    print(classifier.score(X_test,Y_test))\n",
    "    print(confusion_matrix(Y_check, Y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikator oparty na sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nie wiem czy coś tu pisać"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast brać słupki z LOB dzielimy cały zakres na kubełki, których wielkość to ułamek wartości mid price. Następnie bierzemy tylko kilka z każdej strony mid price i normujemy tak, aby wartości sumowały się do jedynki.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fits(index, arr):\n",
    "    return index >= 0 and index < len(arr)\n",
    "\n",
    "\n",
    "def get_XY(data, n_buckets=5, bucket_size=0.05 , omit_no_change=True):\n",
    "    \n",
    "    keys = list(data.keys())\n",
    "    keys.sort()\n",
    "\n",
    "    growths = []\n",
    "    X = []\n",
    "\n",
    "    for i, curr in enumerate(keys[:-1]):\n",
    "        currKey = curr\n",
    "        nextKey = keys[i+1]\n",
    "\n",
    "        if not omit_no_change or data[nextKey][2] != data[currKey][2]:\n",
    "            rows = np.zeros(2*n_buckets)\n",
    "            mid_price = data[currKey][2]\n",
    "            #print(mid_price)\n",
    "            for bid_price, bid_size in reversed(data[currKey][0]):\n",
    "                bucket = int(( n_buckets*bucket_size -  (mid_price-bid_price)/mid_price)/bucket_size)\n",
    "                #print(bucket)\n",
    "                if fits(bucket, rows):\n",
    "                    rows[bucket] += bid_size\n",
    "\n",
    "            for ask_price, ask_size in data[currKey][1]:\n",
    "                bucket = int(((ask_price - mid_price)/mid_price )/bucket_size)\n",
    "                #print(bucket + n_buckets)\n",
    "                if fits(bucket + n_buckets, rows):\n",
    "                    rows[bucket + n_buckets] += ask_size\n",
    "\n",
    "            ###print(\"!!!\", currKey, nextKey)\n",
    "\n",
    "            # poprawne dane - min ask > max bid\n",
    "            if data[currKey][0][-1][0] <  data[currKey][1][0][0] and data[nextKey][0][-1][0] <  data[nextKey][1][0][0]:\n",
    "                growths.append(data[currKey][2] < data[nextKey][2])\n",
    "                rows /= rows.sum()\n",
    "                X.append(rows)\n",
    "                \n",
    "    return np.array(X, dtype=np.float32), np.array(growths, dtype=np.int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do trenowania używamy klasycznej sieci neuronowej z warstwami fc i dodatkowo dropoutem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size = 0.01\n",
    "input_size = 2*20\n",
    "hidden_size = 100\n",
    "\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.Tanh(),\n",
    "    nn.Dropout(p=0.6, inplace=False),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.Tanh(),\n",
    "    nn.Dropout(p=0.6, inplace=False),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.Tanh(),\n",
    "    nn.Dropout(p=0.6, inplace=False),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.Tanh(),\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden_size, 2),\n",
    "    nn.LogSoftmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931143999099731\n",
      "Acc: 0.5016279574560452\n",
      "Confusion matrix: \n",
      "[[2311 2296]\n",
      " [   0    0]]\n",
      "\n",
      "Loss: 0.6932200193405151\n",
      "Acc: 0.4922246220302376\n",
      "Confusion matrix: \n",
      "[[2279 2351]\n",
      " [   0    0]]\n",
      "\n",
      "Loss: 0.6931547522544861\n",
      "Acc: 0.49809031678274546\n",
      "Confusion matrix: \n",
      "[[2217 2234]\n",
      " [   0    0]]\n",
      "\n",
      "Loss: 0.69325852394104\n",
      "Acc: 0.48873591989987486\n",
      "Confusion matrix: \n",
      "[[2343 2451]\n",
      " [   0    0]]\n",
      "\n",
      "Loss: 0.6931905150413513\n",
      "Acc: 0.4952051145444859\n",
      "Confusion matrix: \n",
      "[[1859 1895]\n",
      " [   0    0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import accuracy\n",
    "\n",
    "\n",
    "for data in file_locs:\n",
    "    X, Y = get_XY(parse(data), n_buckets=input_size//2, bucket_size=bucket_size)\n",
    "    model.load_state_dict(torch.load(\"models/model\"))\n",
    "    model.eval()\n",
    "    X = torch.from_numpy(X)\n",
    "    Y = torch.from_numpy(Y)\n",
    "    pred = model(X)\n",
    "    loss_val = loss(pred, Y)\n",
    "    print(\"Loss: {}\".format(loss_val.item()))\n",
    "    print(\"Acc: {}\".format(accuracy(pred.data.numpy(), Y.data.numpy())))\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(confusion_matrix(np.argmax(pred.data.numpy(), axis=1), Y.data.numpy()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dziękujemy za uwagę"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
